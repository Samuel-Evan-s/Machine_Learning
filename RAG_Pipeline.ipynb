{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading using PyMuPDF with LlamaIndex"
      ],
      "metadata": {
        "id": "oLNYEAoLj6MV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lCOdaB6jm67",
        "outputId": "a41f4c43-f43d-4fdf-812f-f5524b2cc95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index llama-index-llms-gemini pymupdf\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Markdown, display\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set up Google API key for Gemini\n",
        "GOOGLE_API_KEY = \"\"  # Replace with your actual API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "# Create a directory for our PDFs if it doesn't exist\n",
        "!mkdir -p sample_docs"
      ],
      "metadata": {
        "id": "NdaUxQQImC3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "def upload_pdf():\n",
        "    \"\"\"Upload a PDF file and return its path.\"\"\"\n",
        "    print(\"Please select a PDF file to upload:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.endswith('.pdf'):\n",
        "            # Save to the sample_docs directory\n",
        "            pdf_path = os.path.join(\"sample_docs\", filename)\n",
        "\n",
        "            # Create directory if it doesn't exist\n",
        "            os.makedirs(\"sample_docs\", exist_ok=True)\n",
        "\n",
        "            # Save the file\n",
        "            with open(pdf_path, 'wb') as f:\n",
        "                f.write(uploaded[filename])\n",
        "\n",
        "            print(f\"PDF saved to {pdf_path}\")\n",
        "            return pdf_path\n",
        "        else:\n",
        "            print(f\"File {filename} is not a PDF. Please upload a PDF file.\")\n",
        "\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "Gbevurr_mepA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to upload your own PDF\n",
        "pdf_path = upload_pdf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "gDboRQj6m0oy",
        "outputId": "7960d75e-cc35-4d7b-c766-0f5a453a766d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please select a PDF file to upload:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-adf2e6d3-55ad-44b0-856d-bc5920a76046\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-adf2e6d3-55ad-44b0-856d-bc5920a76046\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving LenderFeesWorksheetNew (2).pdf to LenderFeesWorksheetNew (2) (1).pdf\n",
            "PDF saved to sample_docs/LenderFeesWorksheetNew (2) (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "\n",
        "    # Extract text from all pages\n",
        "    text = \"\\n\".join([page.get_text() for page in doc])\n",
        "\n",
        "    # Print some stats\n",
        "    print(f\"PDF: {pdf_path}\")\n",
        "    print(f\"Number of pages: {len(doc)}\")\n",
        "    print(f\"Extracted {len(text.split())} words from the PDF.\")\n",
        "\n",
        "    # Close the document\n",
        "    doc.close()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "yConugIknEkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if pdf_path:\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    print(\"Extracted Text:\")\n",
        "    print(text[:500])\n",
        "else:\n",
        "    print(\"No PDF file selected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl6T2kqAnbay",
        "outputId": "2f95a726-0985-4534-db47-085f6521c8d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF: sample_docs/LenderFeesWorksheetNew (2) (1).pdf\n",
            "Number of pages: 1\n",
            "Extracted 404 words from the PDF.\n",
            "Extracted Text:\n",
            "Your actual rate, payment, and cost could be higher. Get an official Loan Estimate before choosing a loan.\n",
            "Fee Details and Summary\n",
            "Applicants:\n",
            "Application No:\n",
            "Date Prepared:\n",
            "Loan Program:\n",
            "Prepared By:\n",
            "THIS IS NOT A GOOD FAITH ESTIMATE (GFE). This \"Fees Worksheet\" is provided for informational purposes ONLY, to assist\n",
            "you in determining an estimate of cash that may be required to close and an estimate of your proposed monthly mortgage \n",
            "payment. Actual charges may be more or less, and your transac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intergrating PyMuPDF with LlamaIndex"
      ],
      "metadata": {
        "id": "UcrbHNrInoSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document\n",
        "from typing import List\n",
        "\n",
        "def load_pdf_with_pymupdf(pdf_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a PDF file using PyMuPDF and convert it to a LlamaIndex\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    documents = []\n",
        "\n",
        "\n",
        "    for i, page in enumerate(doc):\n",
        "        text = page.get_text()\n",
        "\n",
        "        if not text.strip():\n",
        "          continue\n",
        "\n",
        "        documents.append(\n",
        "            Document(\n",
        "                text=text,\n",
        "                metadata={\n",
        "                    \"file_name\": os.path.basename(pdf_path),\n",
        "                    \"page_number\": i + 1,\n",
        "                    \"total_pages\": len(doc),\n",
        "                },\n",
        "            )\n",
        "        )\n",
        "\n",
        "    doc.close()\n",
        "\n",
        "    print(f'Processed {pdf_path}')\n",
        "    print(f'Extracted {len(documents)} documents from {pdf_path}')\n",
        "    return documents\n",
        "\n"
      ],
      "metadata": {
        "id": "22iOPgihnwDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_docs = load_pdf_with_pymupdf(pdf_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMCYshI-pJ2p",
        "outputId": "f56dca0e-300b-4156-fff0-232aafb5cfaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sample_docs/LenderFeesWorksheetNew (2) (1).pdf\n",
            "Extracted 1 documents from sample_docs/LenderFeesWorksheetNew (2) (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Initialize Gemini LLM\n",
        "llm = Gemini(model=\"models/gemini-2.5-flash\")\n",
        "Settings.llm = llm\n",
        "\n",
        "# Initialize embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "def process_and_index_pdf(pdf_path):\n",
        "    \"\"\"Process a PDF and create both vector and keyword indices.\"\"\"\n",
        "    # Load documents\n",
        "    documents = load_pdf_with_pymupdf(pdf_path)\n",
        "\n",
        "    # Create vector index\n",
        "    vector_index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "    print(f\"Indexed {len(documents)} document chunks\")\n",
        "\n",
        "    return vector_index\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "Jw4Gv5l6pqGF",
        "outputId": "f62ae93d-9a82-47f5-ba4c-963f78ed4ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-94570297.py:7: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
            "  llm = Gemini(model=\"models/gemini-2.5-flash\")\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(process_and_index_pdf(pdf_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKh9g_uCqoJi",
        "outputId": "9478f033-1aec-47ba-b20d-e0aac70f0168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sample_docs/LenderFeesWorksheetNew (2) (1).pdf\n",
            "Extracted 1 documents from sample_docs/LenderFeesWorksheetNew (2) (1).pdf\n",
            "Indexed 1 document chunks\n",
            "<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x7eff46314c80>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Expansion and Rewriting"
      ],
      "metadata": {
        "id": "pD2fxE2RqwpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Initialize Gemini LLM\n",
        "llm = Gemini(model=\"models/gemini-2.5-flash\")\n",
        "Settings.llm = llm\n",
        "\n",
        "def expand_query(query: str, num_expansions: int = 3) -> list:\n",
        "    prompt = f\"\"\"\n",
        "    I need to search a legal contract with this query: \"{query}\"\n",
        "\n",
        "    Please help me expand this query by generating {num_expansions} alternative versions that:\n",
        "    1. Use different but related terminology\n",
        "    2. Include relevant legal terms that might appear in a contract\n",
        "    3. Cover similar concepts but phrased differently\n",
        "\n",
        "    Format your response as a list of alternative queries only, with no additional text.\n",
        "    \"\"\"\n",
        "    response = llm.complete(prompt)\n",
        "\n",
        "    expanded_queries = [line.strip()  for line in response.split('\\n') if line.strip()]\n",
        "\n",
        "    return expanded_queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "gtAKUklRq1jM",
        "outputId": "4581fb01-65d7-498a-92f8-3baf5d460a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2459431106.py:5: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
            "  llm = Gemini(model=\"models/gemini-2.5-flash\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Query Expansion Using LlamaIndex"
      ],
      "metadata": {
        "id": "rH3AAPxohTh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "\n",
        "# Function to create a query engine that uses query expansion\n",
        "def create_query_expansion_engine(index):\n",
        "    \"\"\"Create a query engine that uses query expansion.\"\"\"\n",
        "    # First create multiple retrievers (base retriever)\n",
        "    base_retriever = index.as_retriever(similarity_top_k=2)\n",
        "\n",
        "    # Create a query fusion retriever\n",
        "    fusion_retriever = QueryFusionRetriever(\n",
        "        retrievers=[base_retriever],\n",
        "        llm=llm,\n",
        "        similarity_top_k=2,\n",
        "        num_queries=3,  # Generate 3 queries per original query\n",
        "        mode=\"reciprocal_rerank\"  # Use reciprocal rank fusion\n",
        "    )\n",
        "\n",
        "    # Create the query engine with the fusion retriever\n",
        "    query_engine = RetrieverQueryEngine.from_args(\n",
        "        retriever=fusion_retriever,\n",
        "        llm=llm,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    return query_engine\n"
      ],
      "metadata": {
        "id": "Q2r6DGEThR4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-retrievers-bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzBNIjK0551F",
        "outputId": "04b03e8b-507b-4926-9072-06dfd476ff9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-retrievers-bm25 in /usr/local/lib/python3.12/dist-packages (0.6.5)\n",
            "Requirement already satisfied: bm25s>=0.2.7.post1 in /usr/local/lib/python3.12/dist-packages (from llama-index-retrievers-bm25) (0.2.14)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.13.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-retrievers-bm25) (0.14.4)\n",
            "Requirement already satisfied: pystemmer<3,>=2.2.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-retrievers-bm25) (2.2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from bm25s>=0.2.7.post1->llama-index-retrievers-bm25) (1.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bm25s>=0.2.7.post1->llama-index-retrievers-bm25) (2.0.2)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.13.0)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.8.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.9.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (10.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.5.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.11.10)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.1.6)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (25.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "\n",
        "def create_hybrid_retriever(index, query, top_k=2):\n",
        "    \"\"\"Create a hybrid retrieval approach combining vector and keyword search.\"\"\"\n",
        "    # Method 1: Vector retrieval (semantic search)\n",
        "    vector_retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "    vector_nodes = vector_retriever.retrieve(query)\n",
        "\n",
        "    # Method 2: BM25 retrieval (keyword-based search)\n",
        "    # Get all nodes from the index\n",
        "    nodes = [node for node in index.docstore.docs.values()]\n",
        "    bm25_retriever = BM25Retriever.from_defaults(\n",
        "        nodes=nodes,\n",
        "        similarity_top_k=top_k\n",
        "    )\n",
        "    keyword_nodes = bm25_retriever.retrieve(query)\n",
        "\n",
        "    # Combine results (simple approach)\n",
        "    all_nodes = []\n",
        "    all_nodes.extend(vector_nodes)\n",
        "    all_nodes.extend(keyword_nodes)\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_nodes = []\n",
        "    seen_ids = set()\n",
        "    for node in all_nodes:\n",
        "        if node.node_id not in seen_ids:\n",
        "            unique_nodes.append(node)\n",
        "            seen_ids.add(node.node_id)\n",
        "\n",
        "    # Sort by score (higher is better)\n",
        "    sorted_nodes = sorted(unique_nodes, key=lambda x: x.score if hasattr(x, 'score') else 0.0, reverse=True)\n",
        "\n",
        "    # Limit to top results\n",
        "    top_nodes = sorted_nodes[:top_k]\n",
        "\n",
        "    return top_nodes"
      ],
      "metadata": {
        "id": "SXhzhc6ajYvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-retrievers-bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHMdGVTKjdMI",
        "outputId": "0557fa5d-8748-4f31-c133-774b5396b5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-retrievers-bm25 in /usr/local/lib/python3.12/dist-packages (0.6.5)\n",
            "Requirement already satisfied: bm25s>=0.2.7.post1 in /usr/local/lib/python3.12/dist-packages (from llama-index-retrievers-bm25) (0.2.14)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.13.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-retrievers-bm25) (0.14.4)\n",
            "Requirement already satisfied: pystemmer<3,>=2.2.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-retrievers-bm25) (2.2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from bm25s>=0.2.7.post1->llama-index-retrievers-bm25) (1.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bm25s>=0.2.7.post1->llama-index-retrievers-bm25) (2.0.2)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.13.0)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.8.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.9.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (10.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.5.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.11.10)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.1.6)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (25.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "\n",
        "def create_hybrid_retriever(index, query, top_k=2):\n",
        "    \"\"\"Create a hybrid retrieval approach combining vector and keyword search.\"\"\"\n",
        "    # Method 1: Vector retrieval (semantic search)\n",
        "    vector_retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "    vector_nodes = vector_retriever.retrieve(query)\n",
        "\n",
        "    # Method 2: BM25 retrieval (keyword-based search)\n",
        "    # Get all nodes from the index\n",
        "    nodes = [node for node in index.docstore.docs.values()]\n",
        "    bm25_retriever = BM25Retriever.from_defaults(\n",
        "        nodes=nodes,\n",
        "        similarity_top_k=top_k\n",
        "    )\n",
        "    keyword_nodes = bm25_retriever.retrieve(query)\n",
        "\n",
        "    # Combine results (simple approach)\n",
        "    all_nodes = []\n",
        "    all_nodes.extend(vector_nodes)\n",
        "    all_nodes.extend(keyword_nodes)\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_nodes = []\n",
        "    seen_ids = set()\n",
        "    for node in all_nodes:\n",
        "        if node.node_id not in seen_ids:\n",
        "            unique_nodes.append(node)\n",
        "            seen_ids.add(node.node_id)\n",
        "\n",
        "    # Sort by score (higher is better)\n",
        "    sorted_nodes = sorted(unique_nodes, key=lambda x: x.score if hasattr(x, 'score') else 0.0, reverse=True)\n",
        "\n",
        "    # Limit to top results\n",
        "    top_nodes = sorted_nodes[:top_k]\n",
        "\n",
        "    return top_nodes"
      ],
      "metadata": {
        "id": "1L8AeVGOiGTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
        "\n",
        "def build_rag_pipeline(index):\n",
        "    \"\"\"Build a simple but effective RAG pipeline with hybrid retrieval and reranking.\"\"\"\n",
        "\n",
        "    # Get all nodes from the index's docstore\n",
        "    nodes = list(index.docstore.docs.values())\n",
        "\n",
        "    # Determine safe top_k value (number of nodes to retrieve)\n",
        "    # Must be at least 1 and no more than the number of available nodes\n",
        "    num_nodes = len(nodes)\n",
        "    safe_top_k = min(2, max(1, num_nodes))\n",
        "\n",
        "    print(f\"Index contains {num_nodes} nodes, using top_k={safe_top_k}\")\n",
        "\n",
        "    # Step 1: Create a hybrid retriever combining vector and keyword search\n",
        "    # First, get the vector retriever (for semantic understanding)\n",
        "    vector_retriever = index.as_retriever(\n",
        "        similarity_top_k=safe_top_k  # Retrieve top 3 most similar chunks\n",
        "    )\n",
        "\n",
        "    # Next, create a BM25 retriever (for keyword matching)\n",
        "    # Get all nodes from the index's docstore\n",
        "    nodes = list(index.docstore.docs.values())\n",
        "    bm25_retriever = BM25Retriever.from_defaults(\n",
        "        nodes=nodes,\n",
        "        similarity_top_k=safe_top_k  # Retrieve top 3 most similar chunks\n",
        "    )\n",
        "\n",
        "    # Create a proper hybrid retriever class\n",
        "    class HybridRetriever(BaseRetriever):\n",
        "        \"\"\"Hybrid retriever that combines vector and keyword search results.\"\"\"\n",
        "\n",
        "        def __init__(self, vector_retriever, keyword_retriever, top_k=2):\n",
        "            \"\"\"Initialize with vector and keyword retrievers.\"\"\"\n",
        "            self.vector_retriever = vector_retriever\n",
        "            self.keyword_retriever = keyword_retriever\n",
        "            self.top_k = top_k\n",
        "            super().__init__()\n",
        "\n",
        "        def _retrieve(self, query_bundle, **kwargs):\n",
        "            \"\"\"Retrieve from both retrievers and combine results.\"\"\"\n",
        "            # Get results from both retrievers\n",
        "            vector_nodes = self.vector_retriever.retrieve(query_bundle)\n",
        "            keyword_nodes = self.keyword_retriever.retrieve(query_bundle)\n",
        "\n",
        "            # Combine all nodes\n",
        "            all_nodes = list(vector_nodes) + list(keyword_nodes)\n",
        "\n",
        "            # Remove duplicates (by node_id)\n",
        "            unique_nodes = {}\n",
        "            for node in all_nodes:\n",
        "                if node.node_id not in unique_nodes:\n",
        "                    unique_nodes[node.node_id] = node\n",
        "\n",
        "            # Sort by score (higher is better)\n",
        "            sorted_nodes = sorted(\n",
        "                unique_nodes.values(),\n",
        "                key=lambda x: x.score if hasattr(x, 'score') else 0.0,\n",
        "                reverse=True\n",
        "            )\n",
        "\n",
        "            return sorted_nodes[:self.top_k]  # Return top results\n",
        "\n",
        "    # Create our hybrid retriever instance\n",
        "    hybrid_retriever = HybridRetriever(\n",
        "        vector_retriever=vector_retriever,\n",
        "        keyword_retriever=bm25_retriever,\n",
        "        top_k=safe_top_k\n",
        "    )\n",
        "\n",
        "    # Step 2: Create a reranker to prioritize the most relevant chunks\n",
        "    # if num_nodes > 1:\n",
        "    #     print(222)\n",
        "    #     reranker = SentenceTransformerRerank(\n",
        "    #         model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    #         top_n=min(2, num_nodes)  # Keep only top results after reranking\n",
        "    #     )\n",
        "\n",
        "    #     node_postprocessors = [reranker]\n",
        "    # else:\n",
        "    #     node_postprocessors = []\n",
        "\n",
        "\n",
        "    # Step 3: Build the query engine\n",
        "    query_engine = RetrieverQueryEngine.from_args(\n",
        "        retriever=hybrid_retriever,\n",
        "        llm=llm,\n",
        "        # node_postprocessors=[reranker]\n",
        "    )\n",
        "\n",
        "    return query_engine"
      ],
      "metadata": {
        "id": "mwn1BayAu4Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = process_and_index_pdf(pdf_path)\n",
        "rag_engine = build_rag_pipeline(index)\n",
        "response = rag_engine.query(\"What are the penalties for late payments?\")\n",
        "print('\\nFinal Response:\\n ---------------------- \\n')\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "30vb9BAO5JWE",
        "outputId": "6e957c80-6bdf-44f2-cab3-f07cfddb2730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:bm25s:Building index from IDs objects\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sample_docs/LenderFeesWorksheetNew (2) (1).pdf\n",
            "Extracted 1 documents from sample_docs/LenderFeesWorksheetNew (2) (1).pdf\n",
            "Indexed 1 document chunks\n",
            "Index contains 1 nodes, using top_k=1\n",
            "\n",
            "Final Response:\n",
            " ---------------------- \n",
            "\n",
            "Information regarding penalties for late payments is not available.\n"
          ]
        }
      ]
    }
  ]
}